{
 "cells": [
  {
   "cell_type": "raw",
   "id": "92e138e0-3d3e-4232-95bd-a43b86e752a7",
   "metadata": {},
   "source": [
    "SQL Solutions\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba5ab5f7-9aef-4c26-ac4e-77df862c70cc",
   "metadata": {},
   "source": [
    "Query 1:\n",
    "\n",
    "Problem: Write an SQL query to return all months in the current year for which there are exactly 30 days.\n",
    "\n",
    "Approach:\n",
    "\n",
    "The tblDimDate table contains information about dates.\n",
    "We'll filter the current year using the iYear column.\n",
    "We group by the month and count the number of days in each month.\n",
    "Select months with exactly 30 days.\n",
    "\n",
    "SELECT sMonth\n",
    "FROM tblDimDate\n",
    "WHERE iYear = YEAR(CURRENT_DATE)\n",
    "GROUP BY iMonth, sMonth\n",
    "HAVING COUNT(dateDay) = 30;\n",
    "\n",
    "Explanation:\n",
    "\n",
    "We use YEAR(CURRENT_DATE) to ensure we only check the current year.\n",
    "We group by iMonth and sMonth to get the count of days for each month.\n",
    "The HAVING clause filters for months with exactly 30 days.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b043b26-7cbb-4f27-81cc-e7365ab9ee6b",
   "metadata": {},
   "source": [
    "QUERY 2:\n",
    "\n",
    "Problem: tblDimDate should have one row (one date) for every date between the first date and the last date in the table. Write a SQL query to determine how many dates are missing, if\n",
    "any, between the first date and last date. You do not need to supply a list of the missing dates.\n",
    "Approach:\n",
    "\n",
    "Use MIN(dateDay) and MAX(dateDay) to get the first and last dates.\n",
    "Calculate the total number of days between these two dates using DATEDIFF.\n",
    "Subtract the actual count of dates in the table from the total calculated days to find the missing dates.\n",
    "\n",
    "SELECT \n",
    "    DATEDIFF(MAX(dateDay), MIN(dateDay)) + 1 - COUNT(dateDay) AS missing_dates\n",
    "FROM tblDimDate;\n",
    "\n",
    "Explanation:\n",
    "\n",
    "DATEDIFF(MAX(dateDay), MIN(dateDay)) + 1 calculates the expected number of dates.\n",
    "Subtracting COUNT(dateDay) gives the number of missing dates.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3cd3e2c-0ac2-45fd-971b-f3afd3352010",
   "metadata": {},
   "source": [
    "QUERY 3:\n",
    "\n",
    "Problem: Write an SQL query to identify all orders scheduled to run in November 2023, for which there are not yet any records in tblAdvertiserLineItem\n",
    "\n",
    "Approach:\n",
    "\n",
    "Filter orders from tblOrder that fall in November 2023 using dateStart and dateEnd.\n",
    "Use a LEFT JOIN with tblAdvertiserLineItem on the foreign key idOrder.\n",
    "Return orders where there is no corresponding entry in tblAdvertiserLineItem.\n",
    "\n",
    "SELECT id, idAdvertiser, sStatus, dateStart, dateEnd\n",
    "FROM tblOrder o\n",
    "LEFT JOIN tblAdvertiserLineItem ali ON o.id = ali.idOrder\n",
    "WHERE o.dateStart >= '2023-11-01' \n",
    "  AND o.dateEnd <= '2023-11-30'\n",
    "  AND ali.idOrder IS NULL;\n",
    "\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The LEFT JOIN ensures that orders without matching records in tblAdvertiserLineItem are included.\n",
    "The condition ali.idOrder IS NULL filters out those without linked line items"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98e61241-2ecb-4b61-836a-0f7a1d285351",
   "metadata": {},
   "source": [
    "QUERY 4:\n",
    "Problem: Write an SQL query to return the number of campaigns in tblOrder grouped by campaign duration.\n",
    "Campaign duration would be the number of days between dateStart and dateEnd.\n",
    "\n",
    "Approach:\n",
    "Use the DATEDIFF function to calculate the campaign duration in days.\n",
    "Group the results by the campaign duration.\n",
    "Count the number of campaigns for each unique duration.\n",
    "\n",
    "SELECT \n",
    "    DATEDIFF(dateEnd, dateStart) + 1 AS campaign_duration,\n",
    "    COUNT(*) AS num_campaigns\n",
    "FROM tblOrder\n",
    "GROUP BY campaign_duration\n",
    "ORDER BY campaign_duration;\n",
    "\n",
    "Explanation:\n",
    "DATEDIFF(dateEnd, dateStart) + 1: The DATEDIFF function returns the difference in days between dateEnd and dateStart. Adding 1 includes both the start and end dates in the duration calculation.\n",
    "GROUP BY campaign_duration: Groups the campaigns based on the calculated duration.\n",
    "COUNT(*): Counts the number of campaigns for each duration.\n",
    "ORDER BY campaign_duration: Sorts the output by the campaign duration in ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc933bc-ef25-41e9-97ae-2d2c1ec7fcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "163050c3-fd33-46e3-a44f-64847e21ee49",
   "metadata": {},
   "source": [
    "Data Validation and Analysis\n",
    "For a project, you are working with structured (tabular) text data which is expected to conform to a specified schema.\n",
    "The schema defines the delimiters used to split data into fields, as well as the the name and data type of data for each field.\n",
    "Can you describe or write a script or pipeline which will determine if a given input file is matching the schema?\n",
    "If there are exceptions, the script should count the frequency of the exceptions, along with one or a few examples"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d192b88-fec8-45be-81ee-7318c533bf19",
   "metadata": {},
   "source": [
    "We will write a Python script to:\n",
    "\n",
    "Load the structured text data.\n",
    "Validate the data against the expected schema.\n",
    "Count and report exceptions, along with examples.\n",
    "\n",
    "\n",
    "Approach Overview\n",
    "\n",
    "Input: A structured text file (e.g., CSV or TSV).\n",
    "Schema Definition: A dictionary specifying the expected data types for each column.\n",
    "Validation:\n",
    "Check if each row matches the schema in terms of data types and column count.\n",
    "Identify any missing fields or mismatches in data types.\n",
    "Output: Summary of validation errors, including:\n",
    "Number of errors per field.\n",
    "Examples of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150a8f5-d2ff-4bf1-98b3-ae05a27a7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define the expected schema as a dictionary where keys are column names\n",
    "# and values are the expected data types\n",
    "expected_schema = {\n",
    "    'field1': str,\n",
    "    'field2': int,\n",
    "    'field3': float,\n",
    "    'field4': str\n",
    "}\n",
    "\n",
    "# Define the delimiter used in the input file (e.g., ',' for CSV or '\\t' for TSV)\n",
    "delimiter = ','\n",
    "\n",
    "def validate_row(row, schema):\n",
    "    \"\"\"\n",
    "    Validate a single row of data against the expected schema.\n",
    "    Returns a dictionary of exceptions, if any.\n",
    "    \"\"\"\n",
    "    exceptions = {}\n",
    "    for field, expected_type in schema.items():\n",
    "        if field not in row:\n",
    "            exceptions[field] = \"Missing field\"\n",
    "        else:\n",
    "            value = row[field]\n",
    "            try:\n",
    "                # Attempt type conversion to validate data type\n",
    "                if expected_type == int:\n",
    "                    int(value)\n",
    "                elif expected_type == float:\n",
    "                    float(value)\n",
    "                elif expected_type == str and not isinstance(value, str):\n",
    "                    raise ValueError(\"Expected string\")\n",
    "            except ValueError:\n",
    "                exceptions[field] = f\"Invalid type: expected {expected_type.__name__}, got {type(value).__name__}\"\n",
    "    return exceptions\n",
    "\n",
    "def validate_file(file_path):\n",
    "    \"\"\"\n",
    "    Validate a CSV/TSV file against the expected schema.\n",
    "    \"\"\"\n",
    "    total_exceptions = {}\n",
    "    example_exceptions = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.DictReader(file, delimiter=delimiter)\n",
    "        for row_number, row in enumerate(reader, start=1):\n",
    "            # Validate each row\n",
    "            exceptions = validate_row(row, expected_schema)\n",
    "            if exceptions:\n",
    "                # Log exceptions and count their occurrences\n",
    "                for field, error in exceptions.items():\n",
    "                    total_exceptions[field] = total_exceptions.get(field, 0) + 1\n",
    "                    # Store the first example of each error type\n",
    "                    if field not in example_exceptions:\n",
    "                        example_exceptions[field] = (row_number, error)\n",
    "    \n",
    "    # Output the summary of exceptions\n",
    "    print(\"\\nValidation Summary:\")\n",
    "    if total_exceptions:\n",
    "        for field, count in total_exceptions.items():\n",
    "            row_number, error_message = example_exceptions[field]\n",
    "            print(f\"Field '{field}': {count} errors\")\n",
    "            print(f\"Example from row {row_number}: {error_message}\")\n",
    "    else:\n",
    "        print(\"No exceptions found. All data is valid.\")\n",
    "\n",
    "# Example usage\n",
    "file_path = 'data.csv'  # Update this with your file path\n",
    "validate_file(file_path)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5fa6aba-2d5f-4e29-b597-6fc82400f2eb",
   "metadata": {},
   "source": [
    "Explanation of the Script\n",
    "\n",
    "Schema Definition:\n",
    "\n",
    "The schema is defined as a dictionary where:\n",
    "Keys represent the field names.\n",
    "Values represent the expected data types (e.g., str, int, float).\n",
    "Row Validation:\n",
    "\n",
    "For each row, the script checks:\n",
    "\n",
    "If all required fields are present.\n",
    "If each field matches the expected data type.\n",
    "Any mismatches or missing fields are recorded as exceptions.\n",
    "Exception Reporting:\n",
    "\n",
    "The script counts the frequency of each type of exception.\n",
    "It also provides one example for each exception type, indicating the row number and the error message.\n",
    "Sample Input (CSV):\n",
    "\n",
    "field1,field2,field3,field4\n",
    "abc,123,45.6,hello\n",
    "def,xyz,78.9,world\n",
    "ghi,456,,example\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "Validation Summary:\n",
    "Field 'field2': 1 errors\n",
    "Example from row 2: Invalid type: expected int, got str\n",
    "Field 'field3': 1 errors\n",
    "Example from row 3: Invalid type: expected float, got str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3274e6-c537-41d9-b7b2-a27e1a425311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8523e71e-a379-483e-94ea-faa46077e974",
   "metadata": {},
   "source": [
    "GBQ"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac024c28-6556-4563-b065-540aa2fe3423",
   "metadata": {},
   "source": [
    "How do we identify where the data is stored in a GBQ table?\n",
    "\n",
    "SELECT \n",
    "    table_catalog,\n",
    "    table_schema,\n",
    "    table_name,\n",
    "    location\n",
    "FROM \n",
    "    `project-id.dataset.INFORMATION_SCHEMA.TABLES`\n",
    "WHERE \n",
    "    table_name = 'auctions';\n",
    "\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The location field will show where the table is stored (e.g., US, EU).\n",
    "Replace project-id.dataset with your actual project and dataset names."
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa54a500-122a-458f-ad63-64df3c542b75",
   "metadata": {},
   "source": [
    "How can we see what partitions the table may have?\n",
    "\n",
    "SELECT \n",
    "    partition_id,\n",
    "    row_count,\n",
    "    total_bytes\n",
    "FROM \n",
    "    `project-id.dataset.INFORMATION_SCHEMA.PARTITIONS`\n",
    "WHERE \n",
    "    table_name = 'auctions';\n",
    "\n",
    "\n",
    "Explanation:\n",
    "\n",
    "This query will list details of each partition, such as the partition ID, row count, and size in bytes.\n",
    "Useful for understanding how your table is partitioned and optimizing performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f9536-a6d7-4cbc-9f58-f3923bb30244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "bb261ca7-6595-47b4-9a6c-93113a2f50bc",
   "metadata": {},
   "source": [
    "Provide a GBQ query to get a distribution of the number of auctions and line items, grouped by the number of segments within each auction record.\n",
    "\n",
    "SELECT \n",
    "    ARRAY_LENGTH(arysegments) AS num_segments,\n",
    "    COUNT(DISTINCT auctionid) AS num_auctions,\n",
    "    COUNT(DISTINCT idlineitem) AS num_line_items\n",
    "FROM \n",
    "    `project-id.dataset.auctions`\n",
    "GROUP BY \n",
    "    num_segments\n",
    "ORDER BY \n",
    "    num_segments;\n",
    "\n",
    "\n",
    "\n",
    "Explanation:\n",
    "\n",
    "ARRAY_LENGTH(arysegments): Calculates the number of segments in each auction.\n",
    "COUNT(DISTINCT auctionid): Counts unique auction IDs.\n",
    "COUNT(DISTINCT idlineitem): Counts unique line item IDs.\n",
    "The query groups by the number of segments and shows the distribution of auctions and line items."
   ]
  },
  {
   "cell_type": "raw",
   "id": "143268fe-20bc-44cf-91ad-d5dafef71601",
   "metadata": {},
   "source": [
    "Provide a GBQ query to get the distinct count of auctions and line items associated with each segment within arysegments.\n",
    "\n",
    "SELECT \n",
    "    segment,\n",
    "    COUNT(DISTINCT auctionid) AS num_auctions,\n",
    "    COUNT(DISTINCT idlineitem) AS num_line_items\n",
    "FROM \n",
    "    `project-id.dataset.auctions`,\n",
    "    UNNEST(arysegments) AS segment\n",
    "GROUP BY \n",
    "    segment\n",
    "ORDER BY \n",
    "    num_auctions DESC;\n",
    "\n",
    "\n",
    "Explanation:\n",
    "\n",
    "UNNEST(arysegments): Expands the arysegments array into individual rows, allowing each segment to be treated as a separate record.\n",
    "The query then groups by each segment and counts distinct auction and line item IDs associated with each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc0831-f69b-4b36-8b1a-4bc78e7f2b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a878b-b430-4c10-ad81-49c7f2ef68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5eb3b579-0c84-401f-9105-54586bf56711",
   "metadata": {},
   "source": [
    "Executes a Hive query for each date in the previous month.\n",
    "The query needs to be run individually for each date due to the constraints on data size and cluster bandwidth.\n",
    "The results should be saved to a single file with 2 columns: utc_date and num_rows.\n",
    "\n",
    "Approach:\n",
    "\n",
    "Calculate all dates from the previous month.\n",
    "Loop through each date, execute the Hive query for that date, and collect the results.\n",
    "Store the results in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a57dbe-1c17-41de-91cf-a9c36cbc9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_dates_last_month():\n",
    "    \"\"\"Generate a list of all dates from the previous month.\"\"\"\n",
    "    today = datetime.today()\n",
    "    # Find the first day of the current month and then subtract a day to get the last day of the previous month\n",
    "    first_day_current_month = today.replace(day=1)\n",
    "    last_day_previous_month = first_day_current_month - timedelta(days=1)\n",
    "    first_day_previous_month = last_day_previous_month.replace(day=1)\n",
    "\n",
    "    # Generate all dates for the previous month\n",
    "    date_list = []\n",
    "    current_date = first_day_previous_month\n",
    "    while current_date <= last_day_previous_month:\n",
    "        date_list.append(current_date.strftime('%Y-%m-%d'))\n",
    "        current_date += timedelta(days=1)\n",
    "    return date_list\n",
    "\n",
    "def run_hive_query(date):\n",
    "    \"\"\"Run the Hive query for a given date.\"\"\"\n",
    "    query = f\"select utc_date, sum(1) as num_rows from my_table where utc_date = '{date}' group by utc_date\"\n",
    "    try:\n",
    "        # Use subprocess to run the Hive query\n",
    "        result = subprocess.check_output(['hive', '-e', query], text=True)\n",
    "        return result.strip()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running query for date {date}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_to_csv(data, output_file):\n",
    "    \"\"\"Save the query results to a CSV file.\"\"\"\n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['utc_date', 'num_rows'])  # Write header\n",
    "        writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    dates = get_dates_last_month()\n",
    "    output_file = 'hive_query_results.csv'\n",
    "    results = []\n",
    "\n",
    "    # Run query for each date and collect results\n",
    "    for date in dates:\n",
    "        result = run_hive_query(date)\n",
    "        if result:\n",
    "            try:\n",
    "                utc_date, num_rows = result.split()\n",
    "                results.append([utc_date, num_rows])\n",
    "            except ValueError:\n",
    "                print(f\"Unexpected format for date {date}: {result}\")\n",
    "\n",
    "    # Save all results to a CSV file\n",
    "    save_to_csv(results, output_file)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "452f8f1a-3e28-41f7-8220-c1cd9938b2db",
   "metadata": {},
   "source": [
    "Explanation of the Script\n",
    "get_dates_last_month():\n",
    "\n",
    "Generates all dates from the previous month using Python's datetime module.\n",
    "run_hive_query(date):\n",
    "\n",
    "Executes the Hive query using subprocess.check_output().\n",
    "The query fetches the total count of rows for each specific date.\n",
    "save_to_csv(data, output_file):\n",
    "\n",
    "Writes the collected data to a CSV file with two columns: utc_date and num_rows.\n",
    "main():\n",
    "\n",
    "Calls the above functions to generate dates, run the Hive query for each date, collect results, and save them to a CSV file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
